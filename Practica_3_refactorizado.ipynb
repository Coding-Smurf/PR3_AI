{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqzHHvtscc-f",
        "outputId": "b6882416-8ef5-498c-c2db-0d3978bc7325"
      },
      "outputs": [],
      "source": [
        "# mount google drive if needed.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# imports\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.preprocessing import image\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from google.colab import files\n",
        "from IPython.display import Image\n",
        "from termcolor import colored\n",
        "from matplotlib import patheffects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# extract the zip file and load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To manage the data set, we will do the following:\n",
        "\n",
        "1. first, we will unzip the dataset and store its contents somewhere we can find.\n",
        "2. After we will create 2 groups of data, training set and test set. Since the contents of the test set provided in the zip file are not labelled, we need to create our own test set from the labelled data.\n",
        "3. after creating the datasets, we will load them into the program so we can use them with our models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set this variable to the path of the zip file\n",
        "zip_file = '/content/drive/MyDrive/data/state-farm-distracted-driver-detection.zip'\n",
        "\n",
        "# set this variable to the path where you want to extract the files\n",
        "path_extracted_files = './extracted_files/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "step 1: extract the files if they havent been extracted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaXyZSW_cc-g"
      },
      "outputs": [],
      "source": [
        "# check if the files have already been extracted\n",
        "if os.path.exists(path_extracted_files):\n",
        "    print(colored('Files have already been extracted', 'green'))\n",
        "else:\n",
        "    print(colored('Extracting files...', 'yellow'))\n",
        "    # if they have not been extracted, extract them\n",
        "    with zipfile.ZipFile(path_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall(path_extracted_files)\n",
        "\n",
        "    print(colored('Files have been extracted!', 'green'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "clear unnecesary files to optimize space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check if we haven't already removed the unnececesary files, and if so remove them\n",
        "print(colored('Removing unnecessary files...', 'yellow'))\n",
        "if os.path.exists(path_extracted_files + 'driver_imgs_list.csv'):\n",
        "    os.remove(path_extracted_files + 'driver_imgs_list.csv')\n",
        "\n",
        "# do the same for the sample_submission file if it exists\n",
        "if os.path.exists(path_extracted_files + 'sample_submission.csv'):\n",
        "    os.remove(path_extracted_files + 'sample_submission.csv')\n",
        "\n",
        "# clear the unlabelled test set that is inside the provided dataset\n",
        "if os.path.exists(path_extracted_files + 'imgs/test'):\n",
        "    shutil.rmtree(path_extracted_files + 'imgs/test')\n",
        "\n",
        "print(colored('Unnecessary files have been removed!', 'green'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "step 2: create the test set from the labelled data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfSP15-wcc-g"
      },
      "outputs": [],
      "source": [
        "#make 2 directories named test and train in the content directory with the same folders that are in the train directory\n",
        "if os.path.exists(path_extracted_files + 'test'):\n",
        "    print(colored('Directories already exist', 'green'))\n",
        "else:\n",
        "    print(colored('Creating class directories...', 'yellow'))\n",
        "    os.makedirs(path_extracted_files + 'test')\n",
        "    os.makedirs(path_extracted_files + 'train')\n",
        "\n",
        "    # Get folders inside 'train'\n",
        "    folders = os.listdir(path_extracted_files + 'imgs/train')\n",
        "\n",
        "    # Create the classes folders in 'test' and 'train'\n",
        "    for folder in folders:\n",
        "        os.makedirs(path_extracted_files + f'test/{folder}')\n",
        "        os.makedirs(path_extracted_files + f'train/{folder}')\n",
        "\n",
        "    print(colored('Class directories have been created!', 'green'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJBm2GZecc-h"
      },
      "outputs": [],
      "source": [
        "# From the initial train directory, generate a new train and test directories, comprising 85% and 15% of the images respectively\n",
        "folders = os.listdir(path_extracted_files + 'imgs/train')\n",
        "\n",
        "# Check if the content/imgs/train/c0 folder is empty\n",
        "if len(os.listdir(path_extracted_files + 'imgs/train/c0')) != 0:\n",
        "\n",
        "    print(colored('Moving images to test and train directories...', 'yellow'))\n",
        "    # Iterate through each folder (c0-c9) in the train directory\n",
        "    for folder in os.listdir(path_extracted_files + 'imgs/train'):\n",
        "\n",
        "        # List all images in the current folder\n",
        "        images = os.listdir(path_extracted_files + f'imgs/train/{folder}')\n",
        "\n",
        "        # Shuffle the list of images randomly\n",
        "        random.shuffle(images)\n",
        "\n",
        "        # Calculate the number of images to move to the test directory (15% of total)\n",
        "        n = int(len(images) * 0.15)\n",
        "\n",
        "        # Select the first n images as test images\n",
        "        test_images = images[:n]\n",
        "\n",
        "        # Move the selected test images to the test directory\n",
        "        for img in test_images:\n",
        "            shutil.move(path_extracted_files + f'imgs/train/{folder}/{img}', path_extracted_files + f'test/{folder}/{img}')\n",
        "\n",
        "    # Move the remaining 85% of the images to the train directory\n",
        "    for folder in os.listdir(path_extracted_files + 'imgs/train'):\n",
        "\n",
        "        # List all images in the current folder\n",
        "        images = os.listdir(path_extracted_files + f'imgs/train/{folder}')\n",
        "\n",
        "        # Shuffle the list of images randomly\n",
        "        random.shuffle(images)\n",
        "\n",
        "        # Move all images to the train directory\n",
        "        for img in images:\n",
        "            shutil.move(path_extracted_files + f'imgs/train/{folder}/{img}', path_extracted_files + f'train/{folder}/{img}')\n",
        "\n",
        "    print(colored('Images have been moved!', 'green'))\n",
        "else:\n",
        "    print(colored('Images have already been moved', 'green'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO4Gcw4qEJpN"
      },
      "source": [
        "step 3: prepare the data for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqUdXHF0cc-h",
        "outputId": "f43c02b2-a70e-41e0-dce5-5969356ef015"
      },
      "outputs": [],
      "source": [
        "# create an unaltered version of the data for training the model\n",
        "unaltered_train_datagen = ImageDataGenerator(validation_split=0.1)\n",
        "\n",
        "unaltered_train_generator = unaltered_train_datagen.flow_from_directory(path + 'train', \n",
        "                                                                        batch_size=128, \n",
        "                                                                        class_mode='categorical', \n",
        "                                                                        color_mode='rgb')\n",
        "\n",
        "# create an unaltered version of the data for testing the model\n",
        "unaltered_test_datagen = ImageDataGenerator()\n",
        "unaltered_test_generator = unaltered_test_datagen.flow_from_directory(path + 'test',\n",
        "                                                                      class_mode='categorical', \n",
        "                                                                      color_mode='rgb')\n",
        "\n",
        "# create a data generator for the augmented data\n",
        "augmented_train_datagen = ImageDataGenerator(validation_split=0.1, \n",
        "                                             target_size=(96,96), \n",
        "                                             rotation_range=20,\n",
        "                                             zoom_range=0.15)\n",
        "\n",
        "augmented_train_generator = augmented_train_datagen.flow_from_directory(path + 'train',\n",
        "                                                                        batch_size=128,\n",
        "                                                                        class_mode='categorical',\n",
        "                                                                        color_mode='rgb')\n",
        "\n",
        "# create a data generator for the augmented data\n",
        "augmented_test_datagen = ImageDataGenerator(target_size=(96,96))\n",
        "\n",
        "augmented_test_generator = augmented_test_datagen.flow_from_directory(path + 'test',\n",
        "                                                                      class_mode='categorical',\n",
        "                                                                      color_mode='rgb')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "select what data we want to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set the data generator to use the training data\n",
        "train_datagen = unaltered_train_datagen\n",
        "test_datagen = unaltered_test_datagen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the input size of the images\n",
        "input_width = train_datagen.target_size[0]\n",
        "input_height = train_datagen.target_size[1]\n",
        "num_colors = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the structure of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "allow_testing = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN4yQ4Ekcc-h"
      },
      "outputs": [],
      "source": [
        "# if we are not testing the model, create a predefined model with the following layers\n",
        "if not allow_testing:\n",
        "\n",
        "# Definition of the network layers\n",
        "layers = [\n",
        "    # First convolution with 16 filters of size 3 x 3 and relu activation function\n",
        "    keras.layers.Conv2D(16,\n",
        "                       (9,9),\n",
        "                       activation='relu',\n",
        "                       input_shape=(input_size,input_height,num_colors)),\n",
        "\n",
        "    keras.layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    # Second convolution with 32 filters of size 3 x 3 and relu activation function\n",
        "    keras.layers.Conv2D(32,\n",
        "                       (3,3),\n",
        "                       activation='relu'),\n",
        "\n",
        "    keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    # Third convolution with 64 filters of size 3 x 3 and relu activation function\n",
        "    keras.layers.Conv2D(64,\n",
        "                       (2,2),\n",
        "                       activation='relu'),\n",
        "\n",
        "    keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    # Flattening to transform the information into a vector\n",
        "    keras.layers.Flatten(),\n",
        "    # Dense layer with 512 neurons and relu activation function\n",
        "    keras.layers.Dense(512,activation='relu'),\n",
        "    keras.layers.Dense(512,activation='relu'),\n",
        "    # Output layer with softmax activation function\n",
        "    keras.layers.Dense(10, activation = tf.nn.softmax),\n",
        "]\n",
        "\n",
        "# Create the model with the layers defined above\n",
        "fixed_model = keras.Sequential(layers, name='Fixed model')\n",
        "\n",
        "# Configure the Adam optimizer with a learning rate of 0.001\n",
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# compile the model and show the summary\n",
        "fixed_model.compile(optimizer=opt,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Network structure\n",
        "fixed_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if we are not testing the hyperparameters, train the model and evaluate it\n",
        "if not allow_testing:\n",
        "    # Train the model using the training set\n",
        "    history = fixed_model.fit(train_generator,\n",
        "                        steps_per_epoch = 4,\n",
        "                        epochs = 50)\n",
        "\n",
        "    # Evaluation of the model using the test set\n",
        "    results = fixed_model.evaluate(test_generator)\n",
        "\n",
        "    # show the results\n",
        "    print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following code block we will the define the possible testing cofigurations that our model will use for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# configure the testing parameters\n",
        "\n",
        "# define the configuration for when we use 3 convolutional layers\n",
        "kernels_3_layers =[\n",
        "    { \"kernels\": [(11,11), (5,5), (3,3)],\n",
        "      \"filters\": [16, 32, 64],\n",
        "    },\n",
        "    { \"kernels\": [(9,9), (5,5), (3,3)],\n",
        "      \"filters\": [16, 32, 64],\n",
        "    },\n",
        "    { \"kernels\": [(7,7), (5,5), (3,3)],\n",
        "      \"filters\": [16, 32, 64],\n",
        "    }\n",
        "]\n",
        "\n",
        "# define the configuration for when we use 2 convolutional layers\n",
        "kernels_2_layers = [\n",
        "    { \"kernels\": [(11,11), (5,5)],\n",
        "      \"filters\": [16, 32],\n",
        "    },\n",
        "    { \"kernels\": [(9,9), (5,5)],\n",
        "      \"filters\": [16, 32],\n",
        "    },\n",
        "    { \"kernels\": [(7,7), (5,5)],\n",
        "      \"filters\": [16, 32],\n",
        "    }\n",
        "]\n",
        "\n",
        "# define the configuration for when we use 4 convolutional layers\n",
        "kernels_4_layers = [\n",
        "    { \"kernels\": [(11,11), (5,5), (3,3), (3,3)],\n",
        "      \"filters\": [16, 32, 64, 128],\n",
        "    },\n",
        "    { \"kernels\": [(9,9), (5,5), (3,3), (3,3)],\n",
        "      \"filters\": [16, 32, 64, 128],\n",
        "    },\n",
        "    { \"kernels\": [(7,7), (5,5), (3,3), (3,3)],\n",
        "      \"filters\": [16, 32, 64, 128],\n",
        "    }\n",
        "]\n",
        "# consolidate the kernel configurations into a single dictionary\n",
        "kernels = {\n",
        "    2: kernels_2_layers,\n",
        "    3: kernels_3_layers,\n",
        "    4: kernels_4_layers\n",
        "}\n",
        "\n",
        "# define the number of convolutional layers\n",
        "convolutional_layers = [2, 3, 4]\n",
        "\n",
        "# define the number of dense layers before the softmax output layer\n",
        "Dense_layers = [1, 2, 3]   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the different hyperarameter testing configurations have been defined, we will try them and store the models so that we can compare between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if we do allow testing, we will test the model with the previously defined configurations\n",
        "if allow_testing:\n",
        "    stored_models_dir = './stored_models/'\n",
        "    results_file = './results.csv'\n",
        "    \n",
        "    # create the directory if it does not exist\n",
        "    if not os.path.exists(stored_models_dir):\n",
        "        os.makedirs(stored_models_dir)\n",
        "    \n",
        "    # if the results file does not exist, create it and write the header\n",
        "    if not os.path.exists(stored_models_dir + results_file):\n",
        "        with open(stored_models_dir + results_file, 'w') as file:\n",
        "            file.write(\"Convolutional Layers, Dense Layers, Kernels, Filters, Accuracy, Loss\\n\")\n",
        "    \n",
        "\n",
        "    # iterate through the number of convolutional layers\n",
        "    for num_conv_layers in convolutional_layers:\n",
        "        # iterate through the number of dense layers\n",
        "        for num_dense_layers in Dense_layers:\n",
        "            # iterate through the kernel configurations\n",
        "            for kernel_config in kernels[num_conv_layers]:\n",
        "                # create the layers for the model\n",
        "                layers = []\n",
        "                # iterate through the kernel configurations\n",
        "                for i in range(num_conv_layers):\n",
        "                    layers.append(keras.layers.Conv2D(kernel_config[\"filters\"][i],\n",
        "                                                       kernel_config[\"kernels\"][i],\n",
        "                                                       activation='relu',\n",
        "                                                       input_shape=(input_width, input_height, num_colors)))\n",
        "                    layers.append(keras.layers.MaxPooling2D(2, 2))\n",
        "                # add the flatten layer\n",
        "                layers.append(keras.layers.Flatten())\n",
        "                # add the dense layers\n",
        "                for i in range(num_dense_layers):\n",
        "                    layers.append(keras.layers.Dense(512, activation='relu'))\n",
        "                # add the output layer\n",
        "                layers.append(keras.layers.Dense(10, activation = tf.nn.softmax))\n",
        "                # create the model\n",
        "                model = keras.Sequential(layers, name=f'{num_conv_layers} conv layers, {num_dense_layers} dense layers')\n",
        "                # compile the model\n",
        "                model.compile(optimizer=opt,\n",
        "                              loss='categorical_crossentropy',\n",
        "                              metrics=['accuracy'])\n",
        "                # train the model\n",
        "                history = model.fit(train_generator,\n",
        "                                    steps_per_epoch = 4,\n",
        "                                    epochs = 50)           \n",
        "\n",
        "                # evaluate the model\n",
        "                results = model.evaluate(test_generator)\n",
        "\n",
        "                # get the loss and accuracy\n",
        "                accuracy = results[1]\n",
        "                loss = results[0]\n",
        "                \n",
        "                # set a name for the model\n",
        "                name = f'acc:{accuracy}, loss:{loss}, {stored_models_dir}{num_conv_layers} conv layers, {num_dense_layers} dense layers, {kernel_config[\"kernels\"]}, {kernel_config[\"filters\"]}.h5'\n",
        "\n",
        "                # save the model\n",
        "                model.save(name)\n",
        "                print(f'Saved: {name}')\n",
        "\n",
        "                #Write the results to a csv file so we can print it later as a table\n",
        "                with open(stored_models_dir + results_file, 'a') as file:\n",
        "                    file.write(f'{num_conv_layers}, {num_dense_layers}, {kernel_config[\"kernels\"]}, {kernel_config[\"filters\"]}, {accuracy}, {loss}\\n')\n",
        "                    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create the confussion matrix for a chosen model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "H73Q9GYYcc-i",
        "outputId": "17e93994-6400-4e8b-effc-4172fff9c443"
      },
      "outputs": [],
      "source": [
        "# check if we want to use a saved model\n",
        "use_saved_model = False\n",
        "\n",
        "# if we want to use a saved model, set the path to the model\n",
        "if use_saved_model:\n",
        "    # set the path to the model\n",
        "    model_path = './stored_models/3 conv layers, 2 dense layers, (9, 9, 3), [16, 32].h5'\n",
        "    \n",
        "    # load the model\n",
        "    model = keras.models.load_model(model_path)\n",
        "    print(f'Loaded model from {model_path}')\n",
        "else:\n",
        "    model = fixed_model\n",
        "    \n",
        "# array for the predictions\n",
        "predictions = []\n",
        "\n",
        "# array for the true labels\n",
        "true_labels = []\n",
        "\n",
        "# Iterate over each batch in the test generator\n",
        "for i in range(len(test_generator)):\n",
        "    # Get batch of data and labels and predict on it\n",
        "    batch_data, batch_labels = test_generator[i]\n",
        "    batch_predictions = model.predict(batch_data, verbose=0)\n",
        "    # Append predictions and true labels\n",
        "    predictions.extend(np.argmax(batch_predictions, axis=1))\n",
        "    true_labels.extend(np.argmax(batch_labels, axis=1))\n",
        "\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "labels = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9']\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "disp.plot(cmap='viridis', text_kw={'size': 20, 'weight': 'bold', 'color': 'white', 'path_effects': [patheffects.withStroke(linewidth=5, foreground='black')]})\n",
        "\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing an uploaded image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "Zw2XAGGZcc-i",
        "outputId": "9af0fdab-24aa-4b27-dfff-317541175f76"
      },
      "outputs": [],
      "source": [
        "label_names = ['safe driving', 'texting - right', 'talking on the phone - right', 'texting - left', 'talking on the phone - left', 'operating the radio', 'drinking', 'reaching behind', 'hair and makeup', 'talking to passenger']\n",
        "minimum_value = 0.78\n",
        "\n",
        "import math\n",
        "\n",
        "# We use the file insertion system of Colab\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename, filedata in uploaded.items():\n",
        "    # Display the image using IPython.display.Image\n",
        "    display(Image(data=filedata, width=300))\n",
        "\n",
        "    # Image path configuration\n",
        "    path = '/content/' + filename\n",
        "\n",
        "    # Preprocess the image\n",
        "    img = image.load_img(path, target_size=(96, 96))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "\n",
        "    # Image transformation into a tensor\n",
        "    image_tensor = np.vstack([x])\n",
        "    #scale the tensor values between 0 and 1\n",
        "    image_tensor = image_tensor / 255.0\n",
        "\n",
        "    # Inference execution\n",
        "    classes = model.predict(image_tensor)\n",
        "\n",
        "    # Get the indices of top three classes\n",
        "    top_three_indices = np.argsort(classes[0])[::-1][:3]\n",
        "\n",
        "    # Print the top three classes and their probabilities\n",
        "    print(f\"{filename} is classified as:\")\n",
        "    for i in top_three_indices:\n",
        "        probability = math.trunc(classes[0][i]*1000000000)/1000000000\n",
        "        print(f\"   - {label_names[i]} with probability {probability}\")\n",
        "\n",
        "    # If no class meets the threshold\n",
        "    if not any(classes[0][i] > minimum_value for i in range(len(classes[0]))):\n",
        "        print(filename + ' is not classified in any class.')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
