{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xqzHHvtscc-f"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.preprocessing import image\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from google.colab import files\n",
        "from IPython.display import Image\n",
        "from termcolor import colored\n",
        "from matplotlib import patheffects\n",
        "from PIL import Image\n",
        "import glob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZHLxmnyfz1m"
      },
      "source": [
        "# extract the zip file and load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPpAB92Hfz1n"
      },
      "source": [
        "To manage the data set, we will do the following:\n",
        "\n",
        "1. first, we will unzip the dataset and store its contents somewhere we can find.\n",
        "2. After we will create 2 groups of data, training set and test set. Since the contents of the test set provided in the zip file are not labelled, we need to create our own test set from the labelled data.\n",
        "3. after creating the datasets, we will load them into the program so we can use them with our models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "\n",
        "!gdown 1D-1uuu2qyGRB5z0FysQg5DotLowsq5pD"
      ],
      "metadata": {
        "id": "nnggfc33sQKT",
        "outputId": "a79fd2eb-02a5-48b8-db13-a0ac51d7fb95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Failed to retrieve file url:\n",
            "\n",
            "\tToo many users have viewed or downloaded this file recently. Please\n",
            "\ttry accessing the file again later. If the file you are trying to\n",
            "\taccess is particularly large or is shared with many people, it may\n",
            "\ttake up to 24 hours to be able to view or download the file. If you\n",
            "\tstill can't access a file after 24 hours, contact your domain\n",
            "\tadministrator.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=1D-1uuu2qyGRB5z0FysQg5DotLowsq5pD\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lvRe52mlWsd3",
        "outputId": "a737584c-4962-4531-9cfd-9915ea45f9e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "\n",
        "!gdown 16FwS0KCnyMT_8lipRhEoIZTSDMoPQW7j"
      ],
      "metadata": {
        "id": "vJer48S9WCTq",
        "outputId": "c20a2814-0a68-45d4-d500-38face49dc0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=16FwS0KCnyMT_8lipRhEoIZTSDMoPQW7j\n",
            "From (redirected): https://drive.google.com/uc?id=16FwS0KCnyMT_8lipRhEoIZTSDMoPQW7j&confirm=t&uuid=2845bbda-15d6-42cc-b6e2-adc8ded0095a\n",
            "To: /content/augmented_models.zip\n",
            "100% 3.02G/3.02G [00:30<00:00, 98.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q7iudJNwfz1n"
      },
      "outputs": [],
      "source": [
        "# set this variable to the path of the zip file\n",
        "zip_file = '/content/drive/MyDrive/LAB03/state-farm-distracted-driver-detection.zip'\n",
        "\n",
        "# set this variable to the path where you want to extract the files\n",
        "path_extracted_files = './extracted_files/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcbao0hyfz1o"
      },
      "source": [
        "step 1: extract the files if they havent been extracted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaXyZSW_cc-g",
        "outputId": "fac3e41f-e91a-4fcf-f1bf-0d156719824e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Files have been extracted!\n"
          ]
        }
      ],
      "source": [
        "# check if the files have already been extracted\n",
        "if os.path.exists(path_extracted_files):\n",
        "    print(colored('Files have already been extracted', 'green'))\n",
        "else:\n",
        "    print(colored('Extracting files...', 'yellow'))\n",
        "    # if they have not been extracted, extract them\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(path_extracted_files)\n",
        "\n",
        "    print(colored('Files have been extracted!', 'green'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if the files have already been extracted\n",
        "if os.path.exists(\"models\"):\n",
        "    print(colored('Files have already been extracted', 'green'))\n",
        "else:\n",
        "    print(colored('Extracting files...', 'yellow'))\n",
        "    # if they have not been extracted, extract them\n",
        "    with zipfile.ZipFile(\"/content/augmented_models.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"models\")\n",
        "\n",
        "    print(colored('Files have been extracted!', 'green'))"
      ],
      "metadata": {
        "id": "cHNcurTkX6R0",
        "outputId": "f14c7805-cb78-436d-d435-7da110e72900",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Files have been extracted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo0eZFpxfz1p"
      },
      "source": [
        "clear unnecesary files to optimize space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3561Klofz1p",
        "outputId": "fcae4118-aa73-402b-8de3-d79f32ed157a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing unnecessary files...\n",
            "Unnecessary files have been removed!\n"
          ]
        }
      ],
      "source": [
        "# check if we haven't already removed the unnececesary files, and if so remove them\n",
        "print(colored('Removing unnecessary files...', 'yellow'))\n",
        "if os.path.exists(path_extracted_files + 'driver_imgs_list.csv'):\n",
        "    os.remove(path_extracted_files + 'driver_imgs_list.csv')\n",
        "\n",
        "# do the same for the sample_submission file if it exists\n",
        "if os.path.exists(path_extracted_files + 'sample_submission.csv'):\n",
        "    os.remove(path_extracted_files + 'sample_submission.csv')\n",
        "\n",
        "# clear the unlabelled test set that is inside the provided dataset\n",
        "if os.path.exists(path_extracted_files + 'imgs/test'):\n",
        "    shutil.rmtree(path_extracted_files + 'imgs/test')\n",
        "\n",
        "print(colored('Unnecessary files have been removed!', 'green'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj0rEbmWfz1p"
      },
      "source": [
        "step 2: create the test set from the labelled data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfSP15-wcc-g",
        "outputId": "e14ac067-8bc0-4b96-8e28-510b928a8658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating class directories...\n",
            "Class directories have been created!\n"
          ]
        }
      ],
      "source": [
        "#make 2 directories named test and train in the content directory with the same folders that are in the train directory\n",
        "if os.path.exists(path_extracted_files + 'test'):\n",
        "    print(colored('Directories already exist', 'green'))\n",
        "else:\n",
        "    print(colored('Creating class directories...', 'yellow'))\n",
        "    os.makedirs(path_extracted_files + 'test')\n",
        "    os.makedirs(path_extracted_files + 'train')\n",
        "\n",
        "    # Get folders inside 'train'\n",
        "    folders = os.listdir(path_extracted_files + 'imgs/train')\n",
        "\n",
        "    # Create the classes folders in 'test' and 'train'\n",
        "    for folder in folders:\n",
        "        os.makedirs(path_extracted_files + f'test/{folder}')\n",
        "        os.makedirs(path_extracted_files + f'train/{folder}')\n",
        "\n",
        "    print(colored('Class directories have been created!', 'green'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJBm2GZecc-h",
        "outputId": "790c86ee-50c2-4099-c62d-af906144b4c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving images to test and train directories...\n",
            "Images have been moved!\n"
          ]
        }
      ],
      "source": [
        "# From the initial train directory, generate a new train and test directories, comprising 85% and 15% of the images respectively\n",
        "folders = os.listdir(path_extracted_files + 'imgs/train')\n",
        "\n",
        "# Check if the content/imgs/train/c0 folder is empty\n",
        "if len(os.listdir(path_extracted_files + 'imgs/train/c0')) != 0:\n",
        "\n",
        "    print(colored('Moving images to test and train directories...', 'yellow'))\n",
        "    # Iterate through each folder (c0-c9) in the train directory\n",
        "    for folder in os.listdir(path_extracted_files + 'imgs/train'):\n",
        "\n",
        "        # List all images in the current folder\n",
        "        images = os.listdir(path_extracted_files + f'imgs/train/{folder}')\n",
        "\n",
        "        # Shuffle the list of images randomly\n",
        "        random.shuffle(images)\n",
        "\n",
        "        # Calculate the number of images to move to the test directory (15% of total)\n",
        "        n = int(len(images) * 0.15)\n",
        "\n",
        "        # Select the first n images as test images\n",
        "        test_images = images[:n]\n",
        "\n",
        "        # Move the selected test images to the test directory\n",
        "        for img in test_images:\n",
        "            shutil.move(path_extracted_files + f'imgs/train/{folder}/{img}', path_extracted_files + f'test/{folder}/{img}')\n",
        "\n",
        "    # Move the remaining 85% of the images to the train directory\n",
        "    for folder in os.listdir(path_extracted_files + 'imgs/train'):\n",
        "\n",
        "        # List all images in the current folder\n",
        "        images = os.listdir(path_extracted_files + f'imgs/train/{folder}')\n",
        "\n",
        "        # Shuffle the list of images randomly\n",
        "        random.shuffle(images)\n",
        "\n",
        "        # Move all images to the train directory\n",
        "        for img in images:\n",
        "            shutil.move(path_extracted_files + f'imgs/train/{folder}/{img}', path_extracted_files + f'train/{folder}/{img}')\n",
        "\n",
        "    print(colored('Images have been moved!', 'green'))\n",
        "else:\n",
        "    print(colored('Images have already been moved', 'green'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO4Gcw4qEJpN"
      },
      "source": [
        "step 3: prepare the data for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqUdXHF0cc-h",
        "outputId": "ab517152-9360-4c82-b10e-61f6d9748fbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 19066 images belonging to 10 classes.\n",
            "Found 3358 images belonging to 10 classes.\n",
            "Found 19066 images belonging to 10 classes.\n",
            "Found 3358 images belonging to 10 classes.\n"
          ]
        }
      ],
      "source": [
        "# get the size of the images to no alter them\n",
        "imgage = path_extracted_files + 'train/c0/img_100026.jpg'\n",
        "\n",
        "# load the image\n",
        "img = Image.open(imgage)\n",
        "\n",
        "# Get the width and height of the image\n",
        "width, height = img.size\n",
        "\n",
        "# if using preprocessing set this value to the target width and height\n",
        "size_of_input = (width,height)\n",
        "\n",
        "# create an unaltered version of the data for training the model\n",
        "unaltered_train_datagen = ImageDataGenerator(validation_split=0.1)\n",
        "\n",
        "unaltered_train_generator = unaltered_train_datagen.flow_from_directory(path_extracted_files + 'train',\n",
        "                                                                        batch_size=128,\n",
        "                                                                        target_size=size_of_input,\n",
        "                                                                        class_mode='categorical',\n",
        "                                                                        color_mode='rgb')\n",
        "\n",
        "# create an unaltered version of the data for testing the model\n",
        "unaltered_test_datagen = ImageDataGenerator()\n",
        "unaltered_test_generator = unaltered_test_datagen.flow_from_directory(path_extracted_files + 'test',\n",
        "                                                                      target_size=size_of_input,\n",
        "                                                                      class_mode='categorical',\n",
        "                                                                      color_mode='rgb')\n",
        "\n",
        "# create a data generator for the augmented data\n",
        "augmented_train_datagen = ImageDataGenerator(validation_split=0.1,\n",
        "                                             rescale=1./255,\n",
        "                                             rotation_range=20,\n",
        "                                             zoom_range=0.15)\n",
        "\n",
        "augmented_train_generator = augmented_train_datagen.flow_from_directory(path_extracted_files + 'train',\n",
        "                                                                        batch_size=128,\n",
        "                                                                        target_size=(96,96),\n",
        "                                                                        class_mode='categorical',\n",
        "                                                                        color_mode='rgb')\n",
        "\n",
        "# create a data generator for the augmented data\n",
        "augmented_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "augmented_test_generator = augmented_test_datagen.flow_from_directory(path_extracted_files + 'test',\n",
        "                                                                      target_size=(96, 96),\n",
        "                                                                      class_mode='categorical',\n",
        "                                                                      color_mode='rgb')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20QtorU2fz1r"
      },
      "source": [
        "# Define the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pfmT26Ifz1s"
      },
      "source": [
        "select what data we want to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LjwL1Knffz1s"
      },
      "outputs": [],
      "source": [
        "# set the data generator to use the training data\n",
        "train_datagen = augmented_train_generator\n",
        "test_datagen = augmented_test_generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os_xWx3Zfz1s"
      },
      "source": [
        "Define the inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3noPa9_jsNZ9"
      },
      "outputs": [],
      "source": [
        "# Define the input size of the images\n",
        "num_colors = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrhQPohcfz1s"
      },
      "source": [
        "Define the structure of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkJV_ipHfz1t"
      },
      "outputs": [],
      "source": [
        "allow_testing = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN4yQ4Ekcc-h"
      },
      "outputs": [],
      "source": [
        "# if we are not testing the model, create a predefined model with the following layers\n",
        "if not allow_testing:\n",
        "\n",
        "  # Definition of the network layers\n",
        "  layers = [\n",
        "      # First convolution with 16 filters of size 3 x 3 and relu activation function\n",
        "      keras.layers.Conv2D(16,\n",
        "                        (9,9),\n",
        "                        activation='relu',\n",
        "                        input_shape=(size_of_input[0],size_of_input[1],num_colors)),\n",
        "\n",
        "      keras.layers.MaxPooling2D(2, 2),\n",
        "\n",
        "      # Second convolution with 32 filters of size 3 x 3 and relu activation function\n",
        "      keras.layers.Conv2D(32,\n",
        "                        (3,3),\n",
        "                        activation='relu'),\n",
        "\n",
        "      keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "      # Third convolution with 64 filters of size 3 x 3 and relu activation function\n",
        "      keras.layers.Conv2D(64,\n",
        "                        (2,2),\n",
        "                        activation='relu'),\n",
        "\n",
        "      keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "      # Flattening to transform the information into a vector\n",
        "      keras.layers.Flatten(),\n",
        "      # Dense layer with 512 neurons and relu activation function\n",
        "      keras.layers.Dense(512,activation='relu'),\n",
        "      keras.layers.Dense(512,activation='relu'),\n",
        "      # Output layer with softmax activation function\n",
        "      keras.layers.Dense(10, activation = tf.nn.softmax),\n",
        "  ]\n",
        "\n",
        "  # Create the model with the layers defined above\n",
        "  fixed_model = keras.Sequential(layers, name='Fixed model')\n",
        "\n",
        "  # Configure the Adam optimizer with a learning rate of 0.001\n",
        "  opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "  # compile the model and show the summary\n",
        "  fixed_model.compile(optimizer=opt,\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  # Network structure\n",
        "  fixed_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9_rz_3kfz1t"
      },
      "outputs": [],
      "source": [
        "# if we are not testing the hyperparameters, train the model and evaluate it\n",
        "if not allow_testing:\n",
        "    # Train the model using the training set\n",
        "    history = fixed_model.fit(train_datagen,\n",
        "                        steps_per_epoch = 4,\n",
        "                        epochs = 50)\n",
        "\n",
        "    # Evaluation of the model using the test set\n",
        "    results = fixed_model.evaluate(test_datagen)\n",
        "\n",
        "    # show the results\n",
        "    print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODFHF56Nfz1t"
      },
      "source": [
        "In the following code block we will the define the possible testing cofigurations that our model will use for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ7khTTZfz1t"
      },
      "outputs": [],
      "source": [
        "# configure the testing parameters\n",
        "\n",
        "# define the configuration for when we use 3 convolutional layers\n",
        "kernels_3_layers =[\n",
        "    { \"kernels\": [(11,11), (5,5), (3,3)],\n",
        "      \"filters\": [16, 32, 64],\n",
        "    },\n",
        "    { \"kernels\": [(9,9), (5,5), (3,3)],\n",
        "      \"filters\": [16, 32, 64],\n",
        "    },\n",
        "    { \"kernels\": [(7,7), (5,5), (3,3)],\n",
        "      \"filters\": [16, 32, 64],\n",
        "    }\n",
        "]\n",
        "\n",
        "# define the configuration for when we use 2 convolutional layers\n",
        "kernels_2_layers = [\n",
        "    { \"kernels\": [(11,11), (5,5)],\n",
        "      \"filters\": [16, 32],\n",
        "    },\n",
        "    { \"kernels\": [(9,9), (5,5)],\n",
        "      \"filters\": [16, 32],\n",
        "    },\n",
        "    { \"kernels\": [(7,7), (5,5)],\n",
        "      \"filters\": [16, 32],\n",
        "    }\n",
        "]\n",
        "\n",
        "# define the configuration for when we use 4 convolutional layers\n",
        "kernels_4_layers = [\n",
        "    { \"kernels\": [(11,11), (5,5), (3,3), (3,3)],\n",
        "      \"filters\": [16, 32, 64, 128],\n",
        "    },\n",
        "    { \"kernels\": [(9,9), (5,5), (3,3), (3,3)],\n",
        "      \"filters\": [16, 32, 64, 128],\n",
        "    },\n",
        "    { \"kernels\": [(7,7), (5,5), (3,3), (3,3)],\n",
        "      \"filters\": [16, 32, 64, 128],\n",
        "    }\n",
        "]\n",
        "# consolidate the kernel configurations into a single dictionary\n",
        "kernels = {\n",
        "    2: kernels_2_layers,\n",
        "    3: kernels_3_layers,\n",
        "    4: kernels_4_layers\n",
        "}\n",
        "\n",
        "# define the number of convolutional layers\n",
        "convolutional_layers = [2, 3, 4]\n",
        "\n",
        "# define the number of dense layers before the softmax output layer\n",
        "Dense_layers = [1, 2, 3]\n",
        "\n",
        "# define the diferent learning rates\n",
        "learning_rates = [0.005, 0.001, 0.0005]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNwReIUFfz1u"
      },
      "source": [
        "Once the different hyperarameter testing configurations have been defined, we will try them and store the models so that we can compare between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwISRPM_fz1u"
      },
      "outputs": [],
      "source": [
        "# if we do allow testing, we will test the model with the previously defined configurations\n",
        "if allow_testing:\n",
        "\n",
        "    total_models = len(convolutional_layers) * len(Dense_layers) * len(kernels_3_layers) * len(learning_rates)\n",
        "\n",
        "    models_tested = 0\n",
        "\n",
        "    stored_models_dir = './stored_models/'\n",
        "    results_file = 'results.csv'\n",
        "\n",
        "    # create the directory if it does not exist\n",
        "    if not os.path.exists(stored_models_dir):\n",
        "        os.makedirs(stored_models_dir)\n",
        "\n",
        "    # if the results file does not exist, create it and write the header\n",
        "    if not os.path.exists(stored_models_dir + results_file):\n",
        "        with open(stored_models_dir + results_file, 'w') as file:\n",
        "            file.write(\"Convolutional Layers; Dense Layers; Kernels; Filters; learning rate; Accuracy; Loss\\n\")\n",
        "\n",
        "\n",
        "    # iterate through the number of convolutional layers\n",
        "    for num_conv_layers in convolutional_layers:\n",
        "        # iterate through the number of dense layers\n",
        "        for num_dense_layers in Dense_layers:\n",
        "            # iterate through the kernel configurations\n",
        "            for kernel_config in kernels[num_conv_layers]:\n",
        "              #iterate through the learning rates to test\n",
        "              for lr in learning_rates:\n",
        "                # create the layers for the model\n",
        "                layers = []\n",
        "                # iterate through the kernel configurations\n",
        "                for i in range(num_conv_layers):\n",
        "                    layers.append(keras.layers.Conv2D(kernel_config[\"filters\"][i],\n",
        "                                                       kernel_config[\"kernels\"][i],\n",
        "                                                       activation='relu',\n",
        "                                                       input_shape=(size_of_input[0],size_of_input[1], num_colors)))\n",
        "                    layers.append(keras.layers.MaxPooling2D(2, 2))\n",
        "                # add the flatten layer\n",
        "                layers.append(keras.layers.Flatten())\n",
        "                # add the dense layers\n",
        "                for i in range(num_dense_layers):\n",
        "                    layers.append(keras.layers.Dense(512, activation='relu'))\n",
        "                # add the output layer\n",
        "                layers.append(keras.layers.Dense(10, activation = tf.nn.softmax))\n",
        "                # create the model\n",
        "                model = keras.Sequential(layers, name='model')\n",
        "\n",
        "\n",
        "                # Configure the Adam optimizer with the current testing learning rate\n",
        "                opt = keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "                # compile the model\n",
        "                model.compile(optimizer=opt,\n",
        "                              loss='categorical_crossentropy',\n",
        "                              metrics=['accuracy'])\n",
        "                # train the model\n",
        "                history = model.fit(train_datagen,\n",
        "                                    steps_per_epoch = 4,\n",
        "                                    epochs = 50,\n",
        "                                    verbose=0)\n",
        "\n",
        "                # evaluate the model\n",
        "                results = model.evaluate(test_datagen)\n",
        "\n",
        "                # get the loss and accuracy\n",
        "                accuracy = results[1]\n",
        "                loss = results[0]\n",
        "\n",
        "                # set a name for the model\n",
        "                name = f'{stored_models_dir}/acc:{accuracy:.5f}, loss:{loss:.5f}/{num_conv_layers} conv layers, {num_dense_layers} dense layers, {kernel_config[\"kernels\"]}, {kernel_config[\"filters\"]}, {lr}.h5'\n",
        "\n",
        "                # save the model\n",
        "                model.save(name)\n",
        "                print(f'Saved: {name}')\n",
        "\n",
        "                #Write the results to a csv file so we can print it later as a table\n",
        "                with open(stored_models_dir + results_file, 'a') as file:\n",
        "                    file.write(f'{num_conv_layers}; {num_dense_layers}; {kernel_config[\"kernels\"]}; {kernel_config[\"filters\"]};{lr}; {accuracy:.5f}; {loss:.5f}\\n')\n",
        "\n",
        "\n",
        "                # show the progress of the testing\n",
        "                print()\n",
        "                models_tested += 1\n",
        "                print(f'{models_tested}/{total_models} models tested')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# calculate the metrics for chosen models"
      ],
      "metadata": {
        "id": "FfArYeIYUvy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(model_str):\n",
        "\n",
        "  '''\n",
        "  Method to calculate the metrics for a given model and store it in a csv file\n",
        "  inputs:\n",
        "  model: Current model to test\n",
        "\n",
        "  output:\n",
        "  none\n",
        "  '''\n",
        "\n",
        "  model = keras.models.load_model(model_str)\n",
        "\n",
        "  predictions = []\n",
        "  true_labels = []\n",
        "\n",
        "  for i in range(len(test_datagen)):\n",
        "    # Get batch of data and labels and predict on it\n",
        "    batch_data, batch_labels = test_datagen[i]\n",
        "    batch_predictions = model.predict(batch_data, verbose=0)\n",
        "    # Append predictions and true labels\n",
        "    predictions.extend(np.argmax(batch_predictions, axis=1))\n",
        "    true_labels.extend(np.argmax(batch_labels, axis=1))\n",
        "\n",
        "  cm = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "  # Convert to numpy array\n",
        "  cm = np.array(cm)\n",
        "\n",
        "  # Calculate total samples\n",
        "  total_samples = np.sum(cm)\n",
        "\n",
        "  # Calculate TP, TN, FP, FN\n",
        "  TP = np.diag(cm)\n",
        "  FP = np.sum(cm, axis=0) - TP\n",
        "  FN = np.sum(cm, axis=1) - TP\n",
        "  TN = total_samples - (TP + FP + FN)\n",
        "\n",
        "  # Calculate precision and recall\n",
        "  precision = TP / (TP + FP)\n",
        "  recall = TP / (TP + FN)\n",
        "\n",
        "  # Calculate average precision and recall\n",
        "  average_precision = np.mean(precision)\n",
        "  average_recall = np.mean(recall)\n",
        "\n",
        "  # get the accuracy and loss\n",
        "  results = model.evaluate(test_datagen, verbose=0)\n",
        "\n",
        "\n",
        "  # Calculate total accuracy\n",
        "  accuracy = results[1]\n",
        "\n",
        "  # calculate miss rate\n",
        "  miss_rate = 1 - accuracy\n",
        "\n",
        "  # calculate loss\n",
        "  loss = results[0]\n",
        "\n",
        "  with open(\"metrics.csv\", 'a') as file:\n",
        "    file.write(f'{os.path.basename(model_str)};{accuracy:.4f};{miss_rate:.4f};{average_recall:.4f};{average_precision:.4f};{loss:.4f}\\n')\n"
      ],
      "metadata": {
        "id": "JPkYXuUzZNTe"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_h5_files(root_dir):\n",
        "    '''\n",
        "    This method is used to find all the h5 files in our models file structure\n",
        "\n",
        "    inputs:\n",
        "    root_dir: directory to crawl and look for the h5 files\n",
        "\n",
        "    output:\n",
        "    List of paths to h5 files\n",
        "    '''\n",
        "\n",
        "    h5_files = []\n",
        "    for root, dirs, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".h5\"):\n",
        "                h5_files.append(os.path.join(root, file))\n",
        "    return h5_files\n",
        "\n",
        "root_directory = \"/content/models\"\n",
        "h5_files = find_h5_files(root_directory)\n",
        "\n",
        "# create a csv file to contain the metrics\n",
        "if not os.path.exists(\"metrics.csv\"):\n",
        "    with open(\"metrics.csv\", 'w') as file:\n",
        "        file.write(\"Model Structure;Accuracy;Miss Rate;Recall;Precision;Loss\\n\")\n",
        "\n",
        "# Calculate metrics for each h5 file\n",
        "for file_path in h5_files:\n",
        "    calculate_metrics(file_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "12UHdHnTU1s2",
        "outputId": "4d9c3a43-ae37-49a2-9696-fc288b1bce5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-e6be6280f1d3>:40: RuntimeWarning: invalid value encountered in divide\n",
            "  precision = TP / (TP + FP)\n",
            "<ipython-input-37-e6be6280f1d3>:40: RuntimeWarning: invalid value encountered in divide\n",
            "  precision = TP / (TP + FP)\n",
            "<ipython-input-37-e6be6280f1d3>:40: RuntimeWarning: invalid value encountered in divide\n",
            "  precision = TP / (TP + FP)\n",
            "<ipython-input-37-e6be6280f1d3>:40: RuntimeWarning: invalid value encountered in divide\n",
            "  precision = TP / (TP + FP)\n",
            "<ipython-input-37-e6be6280f1d3>:40: RuntimeWarning: invalid value encountered in divide\n",
            "  precision = TP / (TP + FP)\n",
            "<ipython-input-37-e6be6280f1d3>:40: RuntimeWarning: invalid value encountered in divide\n",
            "  precision = TP / (TP + FP)\n",
            "<ipython-input-37-e6be6280f1d3>:40: RuntimeWarning: invalid value encountered in divide\n",
            "  precision = TP / (TP + FP)\n",
            "<ipython-input-37-e6be6280f1d3>:40: RuntimeWarning: invalid value encountered in divide\n",
            "  precision = TP / (TP + FP)\n",
            "<ipython-input-37-e6be6280f1d3>:40: RuntimeWarning: invalid value encountered in divide\n",
            "  precision = TP / (TP + FP)\n",
            "<ipython-input-37-e6be6280f1d3>:40: RuntimeWarning: invalid value encountered in divide\n",
            "  precision = TP / (TP + FP)\n",
            "<ipython-input-37-e6be6280f1d3>:40: RuntimeWarning: invalid value encountered in divide\n",
            "  precision = TP / (TP + FP)\n",
            "<ipython-input-37-e6be6280f1d3>:40: RuntimeWarning: invalid value encountered in divide\n",
            "  precision = TP / (TP + FP)\n",
            "<ipython-input-37-e6be6280f1d3>:40: RuntimeWarning: invalid value encountered in divide\n",
            "  precision = TP / (TP + FP)\n",
            "<ipython-input-37-e6be6280f1d3>:40: RuntimeWarning: invalid value encountered in divide\n",
            "  precision = TP / (TP + FP)\n",
            "<ipython-input-37-e6be6280f1d3>:40: RuntimeWarning: invalid value encountered in divide\n",
            "  precision = TP / (TP + FP)\n",
            "<ipython-input-37-e6be6280f1d3>:40: RuntimeWarning: invalid value encountered in divide\n",
            "  precision = TP / (TP + FP)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19Ud8Bzkfz1u"
      },
      "source": [
        "# Create the confussion matrix for a chosen model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H73Q9GYYcc-i"
      },
      "outputs": [],
      "source": [
        "# check if we want to use a saved model\n",
        "use_saved_model = False\n",
        "\n",
        "# if we want to use a saved model, set the path to the model\n",
        "if use_saved_model:\n",
        "    # set the path to the model\n",
        "    model_path = './stored_models/3 conv layers, 2 dense layers, (9, 9, 3), [16, 32].h5'\n",
        "\n",
        "    # load the model\n",
        "    model = keras.models.load_model(model_path)\n",
        "    print(f'Loaded model from {model_path}')\n",
        "else:\n",
        "    model = fixed_model\n",
        "\n",
        "# array for the predictions\n",
        "predictions = []\n",
        "\n",
        "# array for the true labels\n",
        "true_labels = []\n",
        "\n",
        "# Iterate over each batch in the test generator\n",
        "for i in range(len(test_generator)):\n",
        "    # Get batch of data and labels and predict on it\n",
        "    batch_data, batch_labels = test_generator[i]\n",
        "    batch_predictions = model.predict(batch_data, verbose=0)\n",
        "    # Append predictions and true labels\n",
        "    predictions.extend(np.argmax(batch_predictions, axis=1))\n",
        "    true_labels.extend(np.argmax(batch_labels, axis=1))\n",
        "\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "labels = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9']\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "disp.plot(cmap='viridis', text_kw={'size': 20, 'weight': 'bold', 'color': 'white', 'path_effects': [patheffects.withStroke(linewidth=5, foreground='black')]})\n",
        "\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7MPsh6lfz1v"
      },
      "source": [
        "# Testing an uploaded image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zw2XAGGZcc-i"
      },
      "outputs": [],
      "source": [
        "label_names = ['safe driving', 'texting - right', 'talking on the phone - right', 'texting - left', 'talking on the phone - left', 'operating the radio', 'drinking', 'reaching behind', 'hair and makeup', 'talking to passenger']\n",
        "minimum_value = 0.78\n",
        "\n",
        "import math\n",
        "\n",
        "# We use the file insertion system of Colab\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename, filedata in uploaded.items():\n",
        "    # Display the image using IPython.display.Image\n",
        "    display(Image(data=filedata, width=300))\n",
        "\n",
        "    # Image path configuration\n",
        "    path = '/content/' + filename\n",
        "\n",
        "    # Preprocess the image\n",
        "    img = image.load_img(path, target_size=(96, 96))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "\n",
        "    # Image transformation into a tensor\n",
        "    image_tensor = np.vstack([x])\n",
        "    #scale the tensor values between 0 and 1\n",
        "    image_tensor = image_tensor / 255.0\n",
        "\n",
        "    # Inference execution\n",
        "    classes = model.predict(image_tensor)\n",
        "\n",
        "    # Get the indices of top three classes\n",
        "    top_three_indices = np.argsort(classes[0])[::-1][:3]\n",
        "\n",
        "    # Print the top three classes and their probabilities\n",
        "    print(f\"{filename} is classified as:\")\n",
        "    for i in top_three_indices:\n",
        "        probability = math.trunc(classes[0][i]*1000000000)/1000000000\n",
        "        print(f\"   - {label_names[i]} with probability {probability}\")\n",
        "\n",
        "    # If no class meets the threshold\n",
        "    if not any(classes[0][i] > minimum_value for i in range(len(classes[0]))):\n",
        "        print(filename + ' is not classified in any class.')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
